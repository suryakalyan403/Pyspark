{"cells":[{"cell_type":"code","source":["\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType,DayTimeIntervalType,TimestampType\nfrom pyspark.sql import functions as SF\n\n# Defining Columns\ncall_category_col = \"call_category\"\npolicy_holder_col = \"policy_holder_id\"\ncall_received_col = \"call_received\"\ndate_format = \"MM-dd-yyyy HH:mm\"\n\n# Sample Data \ncallers_data = [(50837000,\"claims\",\"03-09-2022 02:51\"),(50837000,\"IT_support\",\"03-12-2022 05:37\"),\\\n              (50837000,\"benefits\",\"05-13-2022 18:19\"),(50936674,\"claims\",\"05-31-2022 07:27\"),\\\n              (50886837,\"IT_support\",\"03-11-2022 03:38\"),(50886837,\"\",\"03-19-2022 10:52\")]\n\n\n# Schema creations\nmySchema = StructType([ StructField(policy_holder_col, IntegerType(), True),\\\n                        StructField(call_category_col, StringType(), True),\\\n                        StructField(call_received_col, StringType(), True)])\n\n#Creating Spark DataFrame\ncallers_df = spark.createDataFrame(data=users_data,schema=mySchema)\n\n# Data Cleaning\ncall_received = SF.to_timestamp(call_received_col, date_format).cast(TimestampType())\ncallers_df = users_df.withColumn(call_received_col, call_received)\ncallers_df.show()\ncallers_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"810c5400-7f80-4d21-b619-fc161423d06a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------------+-------------+-------------------+\n|policy_holder_id|call_category|      call_received|\n+----------------+-------------+-------------------+\n|        50837000|       claims|2022-03-09 02:51:00|\n|        50837000|   IT_support|2022-03-12 05:37:00|\n|        50837000|     benefits|2022-05-13 18:19:00|\n|        50936674|       claims|2022-05-31 07:27:00|\n|        50886837|   IT_support|2022-03-11 03:38:00|\n|        50886837|             |2022-03-19 10:52:00|\n+----------------+-------------+-------------------+\n\nroot\n |-- policy_holder_id: integer (nullable = true)\n |-- call_category: string (nullable = true)\n |-- call_received: timestamp (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#\ndays_col = \"days\"\njoin_type = \"inner\"\nfinal_col = \"patient_count\"\npolicy_holder_id2 = \"policy_holder_id2\"\ncall_received_col2 = \"call_received2\"\ncallers_df2 = users_df\ncallers_df2 = callers_df2.select(SF.col(policy_holder_col).alias(policy_holder_id2),\n                          call_category_col,\n                          SF.col(call_received_col).alias(call_received_col2))\n\njoin_cond1 = users_df[policy_holder_col] == callers_df2[policy_holder_id2]\njoin_cond2 = callers_df2[call_received_col2] > users_df[call_received_col]\ndateDiff = SF.datediff(SF.col(call_received_col2),SF.col(call_received_col))\n\nresult_df = users_df.join(callers_df2,on=[join_cond1,join_cond2],how = join_type)\\\n                    .withColumn(days_col, dateDiff)\\\n                    .filter(SF.col(days_col) <7)\\\n                    .agg(SF.countDistinct(col(policy_holder_col)).alias(final_col)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4c3d149a-05b4-415b-bfd5-84be081e77c4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+\n|patient_count|\n+-------------+\n|            1|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"270011ff-8b8a-4da5-a32b-f850ce2fe704","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-question1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1381200926337758}},"nbformat":4,"nbformat_minor":0}
